{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport os\nimport glob\nimport random\nimport PIL.Image as Image\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Architecture","metadata":{}},{"cell_type":"code","source":"class Down(nn.Module):\n    def __init__(self,\n                 in_planes : int,\n                 out_planes: int,\n                 kernel_size : int = 4,\n                 stride : int = 2,\n                 padding : int = 1,\n                 bias : bool = False,\n                 normalize : bool = True,\n                 dropout :bool = False,\n                ) -> None:\n        super(Down, self).__init__()\n        \n        block = [nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, bias = bias)]\n        if normalize:\n            block.append(nn.BatchNorm2d(out_planes))\n        block.append(nn.LeakyReLU(0.2))\n        if dropout:\n            block.append(nn.Dropout(0.5))\n            \n        self.block = nn.Sequential(*block)\n            \n    def forward(self,\n                x : Tensor,\n               ) -> nn.Sequential:\n        \n        return self.block(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Up(nn.Module):\n    def __init__(self,\n                 in_planes : int,\n                 out_planes : int,\n                 kernel_size : int = 4,\n                 stride : int = 2,\n                 padding : int = 1,\n                 bias : bool = False,\n                 dropout : bool = False,\n                ) -> None:\n        super(Up, self).__init__()\n        \n        block = [nn.ConvTranspose2d(in_planes,\n                                    out_planes,\n                                    kernel_size,\n                                    stride,\n                                    padding,\n                                    bias = bias)]\n        block.append(nn.BatchNorm2d(out_planes))\n        block.append(nn.LeakyReLU(0.2))\n        \n        if dropout:\n            block.append(nn.Dropout(0.5))\n            \n        self.block = nn.Sequential(*block)\n    \n    def forward(self,\n                x : Tensor,\n                skip : Tensor,\n               ) -> nn.Sequential:\n        \n        x = self.block(x)\n        x = torch.cat((x, skip), 1)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, \n                 in_planes : int = 3,\n                 planes : int = 64,\n                 normalize : bool = True,\n                ) -> None:\n        super(Generator, self).__init__()\n        ##########################################\n        # Downward layers\n        #########################################\n        \n        self.down1 = Down(in_planes, planes, normalize = False)\n        \n        self.down2 = Down(planes, planes * 2)\n        self.down3 = Down(planes * 2, planes * 4)\n        \n        self.down4 = Down(planes * 4, planes * 8, dropout = True)\n        self.down5 = Down(planes * 8, planes * 8, dropout = True)\n        self.down6 = Down(planes * 8, planes * 8, dropout = True)\n        self.down7 = Down(planes * 8, planes * 8, dropout = True)\n        \n        self.down8 = Down(planes * 8,\n                             planes * 8,\n                             normalize = False, \n                             dropout = True)\n        \n        #########################################\n        # Upward layers\n        ########################################\n        self.up1 = Up(planes * 8, planes * 8, dropout = True)\n        self.up2 = Up(planes * 16, planes * 8, dropout = True)\n        self.up3 = Up(planes * 16, planes * 8, dropout = True)\n        self.up4 = Up(planes * 16, planes * 8, dropout = True)\n        self.up5 = Up(planes * 16, planes * 4)\n        self.up6 = Up(planes * 8, planes * 2)\n        self.up7 = Up(planes * 4, planes)\n        \n        # Final Layer\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(planes * 2, in_planes, 4, 2, 1, bias = True),\n            nn.Tanh()\n        )\n        \n    def forward(self, x):\n        down1 = self.down1(x)\n        down2 = self.down2(down1)\n        down3 = self.down3(down2)\n        down4 = self.down4(down3)\n        down5 = self.down5(down4)\n        down6 = self.down6(down5)\n        down7 = self.down7(down6)\n        down8 = self.down8(down7)\n        \n        up1 = self.up1(down8, down7)\n        up2 = self.up2(up1, down6)\n        up3 = self.up3(up2, down5)\n        up4 = self.up4(up3, down4)\n        up5 = self.up5(up4, down3)\n        up6 = self.up6(up5, down2)\n        up7 = self.up7(up6, down1)\n        \n        final = self.final(up7)\n        return final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, \n                 in_planes : int = 3,\n                 planes : int = 64,\n                ) -> None:\n        super(Discriminator, self).__init__()\n        \n        def blocks(in_planes, planes, normalize = True):\n            block = [nn.Conv2d(in_planes, planes, 4, 2, 1, bias = False)]\n            if normalize:\n                block.append(nn.BatchNorm2d(planes))\n            block.append(nn.ReLU(True))\n            \n            return block\n        \n        self.block = nn.Sequential(\n            *blocks(in_planes * 2, planes, normalize = False),\n            *blocks(planes, planes * 2),\n            *blocks(planes * 2, planes * 4),\n            *blocks(planes * 4, planes * 8),\n            nn.Conv2d(512, 1, 4, 1, 1),\n        )\n        \n    def forward(self, x, y):\n        x = torch.cat([x, y], dim = 1)\n        x = self.block(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class Pix2PixDataset(Dataset):\n    \"\"\"\n    Dataset is taken from kaggle.\n    Here is the link : https://www.kaggle.com/datasets/vikramtiwari/pix2pix-dataset\n    \n    This custom dataset is particular make for citycapes and facades data. If your want to\n    train on other dataset like maps and edges to shoes from the link, you have to make\n    appropriate changes. Also don't forget to check the shape of image data.\n        \n    \"\"\"\n    def __init__(self,\n                root : str,\n                dataset_name_list : list,\n                 mode : str = \"train\",\n                 shape : int = 256,\n                ):\n        super(Pix2PixDataset, self).__init__()\n        \n        def image_list(root, dataset_name_list, mode):\n            image_list = []\n            for i in range(len(dataset_name_list)):\n                path = os.path.join(root, dataset_name_list[i], dataset_name_list[i], mode)\n                for j in glob.glob(path+\"/\"+\"*.jpg\"):\n                    image_list.append(j)\n            return image_list\n        \n        self.image_list = image_list(root, dataset_name_list, mode)\n        \n        self.transform = transforms.Compose([\n            transforms.Resize((shape, shape), Image.BICUBIC),\n            transforms.ToTensor()\n        ])\n        \n    def image_separate(self, image):\n        image = np.array(image, dtype = np.uint8)\n        height, width, channel = image.shape\n        width = int(width / 2)\n        Input =  Image.fromarray(image[:, width: , :])\n        Target = Image.fromarray(image[:, :width , :])\n        \n        return Input, Target\n    \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self, index : int):\n        image = Image.open(self.image_list[index])\n        input_image, target_image = self.image_separate(image)\n        input_image = self.transform(input_image)\n        target_image = self.transform(target_image)\n        return input_image, target_image\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def build_model(args):\n    generator = Generator().to(args.device)\n    discriminator = Discriminator().to(args.device)\n    \n    return generator, discriminator\n    \n    \ndef load_dataset(args, mode):\n    \n    dataloader = DataLoader(Pix2PixDataset(args.root, \n                                           args.dataset_name_list, mode, args.shape),\n                           batch_size = args.batch_size,\n                            shuffle = True,\n                           num_workers = args.num_workers,)\n    return dataloader\n\ndef loss_function(args):\n    bce_loss = nn.BCEWithLogitsLoss().to(args.device)\n    l1_loss = nn.L1Loss().to(args.device)\n    return bce_loss, l1_loss\n\ndef optimizer(generator, discriminator, args):\n    \n    optimizer_g = optim.Adam(generator.parameters(),\n                             lr = args.lr, betas = (args.b1, args.b2))\n    optimizer_d = optim.Adam(discriminator.parameters(),\n                             lr = args.lr, betas = (args.b1, args.b2))\n    \n    return optimizer_g, optimizer_d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"def trainer(loader,\n            generator,\n            discriminator,\n            bce_loss,\n            l1_loss,\n            optimizer_g,\n            optimizer_d,\n            args):\n    \n    ######################\n    # Traning Pix2Pix gan\n    ######################\n    \n    # List for ploting generative losses and discriminator losses\n    g_losses = []\n    d_losses = []\n    \n    start = datetime.now()\n    \n    # Set models in training mode\n    generator.train()\n    discriminator.train()\n    \n    for epoch in range(1, args.epochs):\n        gepoch_loss=0.0\n        depoch_loss=0.0\n        \n        epoch_time = datetime.now()\n        for index, (inputs, targets) in enumerate(loader):\n            b_size = inputs.size(0)\n            \n            inputs = inputs.to(args.device)\n            targets = targets.to(args.device)\n            \n            # Generate images\n            generated_image = generator(inputs)\n            \n            \n\n            ############################\n            # Train discriminator model\n            ############################\n            \n            # Initialize discriminator model with zero gradient\n            discriminator.zero_grad()\n            # Calculate loss of discriminator model on input images\n            disc_real = discriminator(inputs, targets)\n            d_real_loss = bce_loss(disc_real, torch.ones_like(disc_real))\n            # Calculate gradient of discriminator on input images\n            d_real_loss.backward()\n            \n            # Calculate loss of discriminator on generated images\n            disc_fake = discriminator(inputs, generated_image.detach())\n            d_fake_loss = bce_loss(disc_fake, torch.zeros_like(disc_fake))\n            # Calculate gradient of discriminator on generated images\n            d_fake_loss.backward()\n            # Sum of calculated losses on both inputs and generated images\n            d_loss = (d_real_loss + d_fake_loss) * 0.5\n            # Update weight of discriminator model\n            optimizer_d.step()\n            \n            ########################\n            # Train Generator model\n            ########################\n            \n            # Initialize generator model with zero gradient\n            generator.zero_grad()\n            # Calculate loss of generator model based on discriminator model\n            gen_output = discriminator(inputs, generated_image)\n            g_bce_loss = bce_loss(gen_output,  torch.ones_like(gen_output))\n            # Calculate pixel-wise loss\n            g_pixel_wise_loss = args.lambda_pixel * l1_loss(generated_image, targets)\n            # Calculate gradient of discriminator\n            g_loss = g_bce_loss + g_pixel_wise_loss\n            g_loss.backward()\n            # Update weight of generator model\n            optimizer_g.step()\n            \n            # batch losses\n            gepoch_loss += g_loss.item()\n            depoch_loss += d_loss.item()\n            \n            # save images to see training stability\n            if index == 0:\n                vutils.save_image(vutils.make_grid(generated_image, normalize = False),\n                                  os.path.join(args.outputs_dir, f\"gen_images_{epoch}.jpg\"))\n                vutils.save_image(vutils.make_grid(targets, normalize = False),\n                                  os.path.join(args.outputs_dir, f\"target_images_{epoch}.jpg\"))\n                \n            # Print the loss function every ten iterations and the last iteration in this epoch.\n            if index % 10 == 0 or index == len(loader):\n                print(f\"Epoch[{epoch:04d}/{args.epochs:04d}]({index:05d}/{len(loader):05d}) \"\n                      f\"D Loss: {d_loss.item():.6f} G Loss: {g_loss.item():.6f} \")\n        # Obtain per epoch losses\n        gepoch_loss = gepoch_loss/len(loader)\n        depoch_loss = depoch_loss/len(loader)\n        g_losses.append(gepoch_loss)\n        d_losses.append(depoch_loss)\n        \n        # Print epoch losses\n        print(f\"D Epoch Loss: {depoch_loss:.6f} G Loss: {gepoch_loss:.6f} \"\n              f\"Epoch training completed in: {datetime.now() - epoch_time}\")\n    \n    print(\"Training completed in: \" + str(datetime.now() - start))\n    \n    return g_losses, d_losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"class Args():\n    outputs_dir = 'result'\n    if not os.path.exists(outputs_dir):\n        os.makedirs(outputs_dir)\n        \n    dataset_name_list = [\"cityscapes\", \"facedes\"]\n    root = \"../input/pix2pix-dataset\"\n    mode_train = \"train\"\n    most_eval = \"val\"\n    shape = 256\n    batch_size = 64\n    num_workers = 2\n    epochs = 200\n    lr = 0.0002\n    b1 = 0.5\n    b2 = 0.999\n    lambda_pixel = 100\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    seed = 42\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    \nargs = Args()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main(args):\n    loader = load_dataset(args, args.mode_train)\n    print('Loaded dataset successfully')\n    generator, discriminator = build_model(args)\n    print('Built model successfully')\n    bce_loss, l1_loss = loss_function(args)\n    print('Define loss function succesfully')\n    optimizer_g, optimizer_d = optimizer(generator, discriminator, args)\n    print('Define all optimization function successfully')\n    \n    # Train \n    g_losses, d_losses = trainer(loader, generator, discriminator,\n                                 bce_loss, l1_loss,\n                                 optimizer_g, optimizer_d, args)\n    \n    plt.figure(figsize=(10,5))\n    plt.title(\"Generator and Discriminator Loss During Training\")\n    plt.plot(g_losses,label=\"G\")\n    plt.plot(d_losses,label=\"D\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main(args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}