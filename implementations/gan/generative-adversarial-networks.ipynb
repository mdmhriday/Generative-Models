{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport random\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import Tensor\n\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Architecture","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self,\n                 latent_space : int = 100,\n                ) -> None:\n        super(Generator, self).__init__()\n    \n        def block(in_channels : int, out_channels : int, batch_norm : bool = True):\n            layers = [nn.Linear(in_channels, out_channels)]\n            if batch_norm:\n                layers.append(nn.BatchNorm1d(out_channels))\n            layers.append(nn.LeakyReLU(0.2, inplace = True))\n            return layers\n\n        # Layers\n        self.layers = nn.Sequential(\n            *block(latent_space, 128, batch_norm = False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n\n            # outputs : channels * image_size * image_size\n            nn.Linear(1024, 1 * 28 * 28),\n            nn.Tanh()\n        )\n    \n    def forward(self, x : Tensor) -> Tensor:\n        out = self.layers(x)\n        out = out.reshape(out.size(0), 1, 28, 28)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self,\n                ) -> None:\n        super(Discriminator, self).__init__()\n        \n        self.layers = nn.Sequential(\n            # input : channels * image size * image size\n            nn.Linear(1 * 28 * 28, 512),\n            nn.LeakyReLU(0.2, inplace = True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace = True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n    def forward(self, x : Tensor) -> Tensor:\n        out = torch.flatten(x, 1)\n        out = self.layers(out)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def build_model(args):\n    generator = Generator(args.latent_space).to(args.device)\n    discriminator = Discriminator().to(args.device)\n    \n    return generator, discriminator\n\ndef load_dataset(batch_size : int):\n    os.makedirs(\"./data/mnist\", exist_ok = True)\n    dataloader = DataLoader(\n        datasets.MNIST(\"./data/mnist\",\n                       train = True,\n                       transform = transforms.Compose([\n                           transforms.Resize(28),\n                           transforms.ToTensor(),\n                           transforms.Normalize([0.5,],[0.5,]),\n                       ]),\n                       download = True,),\n    batch_size = batch_size,\n    shuffle = True,)\n    \n    return dataloader\n\ndef loss_function(args):\n    adversarial_loss = nn.BCELoss().to(args.device)\n    \n    return adversarial_loss\n    \ndef optimizer(generator, discriminator, args):\n    optimizer_g = optim.Adam(generator.parameters(), lr = args.lr, betas = (args.b1, args.b2))\n    optimizer_d = optim.Adam(discriminator.parameters(), lr = args.lr, betas = (args.b1, args.b2))\n    \n    return optimizer_g, optimizer_d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trainer","metadata":{}},{"cell_type":"code","source":"def trainer(loader, generator, discriminator,optimizer_g, optimizer_d,loss_fn, args):\n    \"\"\" Training Generative Adversarial Networks \"\"\"\n    \n    # list to plot losses\n    g_losses = []\n    d_losses = []\n    \n    start = datetime.now()\n    \n    # set model in training model\n    generator.train()\n    discriminator.train()\n    \n\n    \n    for epoch in range(1, args.epochs):\n        for index, (real, _) in enumerate(loader):\n            #flatten\n            real = real.view(-1, 784).to(args.device)\n            size = real.size(0)\n            # create ground truth. set real sample level to 1 and fake sample level to 0\n            real_sample = torch.full([size, 1], 1.0, dtype=real.dtype, device=args.device)\n            fake_sample = torch.full([size, 1], 0.0, dtype=real.dtype, device=args.device)\n            \n            # create a noise sample for generator input\n            noise = torch.randn([size, args.latent_space], device = args.device)\n            \n            \"\"\" Train Discriminator\"\"\"\n            # initialize discriminator model gradients\n            discriminator.zero_grad()\n            # calculate loss of discriminator model on real images\n            output = discriminator(real)\n            loss_real_d = loss_fn(output, real_sample)\n            # calculate gradient of discriminator in backward pass\n            loss_real_d.backward()\n            D_x = output.mean().item()\n            \n            # generate fake image\n            fake = generator(noise)\n            # calculate loss of discriminator model on fake images\n            output = discriminator(fake.detach())\n            loss_fake_d = loss_fn(output, fake_sample)\n            # calculate gradient of discriminator for this batch summed with previous gradient\n            loss_fake_d.backward()\n            D_G_z1 = output.mean().item()\n            # calculate loss of discriminator model as sum on both real images and fake images\n            loss_d = loss_real_d + loss_fake_d\n            # update weight of discriminator model\n            optimizer_d.step()\n            \n            \"\"\" Train Generator \"\"\"\n            # initialize generator model gradient\n            generator.zero_grad()\n            # calculate the loss of discriminator model on fake images\n            output = discriminator(fake)\n            loss_g = loss_fn(output, real_sample)\n            # calculate gradient of generator\n            loss_g.backward()\n            D_G_z2 = output.mean().item()\n            # update weight of generator model\n            optimizer_g.step()\n            \n            # save images to see training stability\n            if index == 0:\n                vutils.save_image(vutils.make_grid(fake.detach().cpu().view(-1, *(1, 28, 28)), normalize = True), os.path.join(args.outputs_dir, f\"fake_image_{epoch}.jpg\"))\n                vutils.save_image(vutils.make_grid(real.detach().cpu().view(-1, *(1, 28, 28)), normalize = True), os.path.join(args.outputs_dir, f\"real_image_{epoch}.jpg\"))\n                \n            # Print the loss function every ten iterations and the last iteration in this epoch.\n            if index % 10 == 0 or index == len(loader):\n                print(f\"Train stage: adversarial \"\n                      f\"Epoch[{epoch:04d}/{args.epochs:04d}]({index:05d}/{len(loader):05d}) \"\n                      f\"D Loss: {loss_d.item():.6f} G Loss: {loss_g.item():.6f} \"\n                      f\"D(D_x): {D_x:.6f} D(D_G_z1)/D(D_G_z2): {D_G_z1:.6f}/{D_G_z2:.6f}.\")\n            \n            # save losses for plotting later\n            g_losses.append(loss_g.item())\n            d_losses.append(loss_d.item())\n\n        if epoch % 10 == 0:\n            # Save the weight of discriminator and generator model under number of epoch name\n            torch.save(discriminator.state_dict(), os.path.join(args.outputs_dir, f\"d_epoch{epoch}.pth\"))\n            torch.save(generator.state_dict(), os.path.join(args.outputs_dir, f\"g_epoch{epoch}.pth\"))\n        \n            \n        print(\"Training complete in: \" + str(datetime.now() - start))\n    \n    # Save final weight of discriminator and generator model\n    torch.save(discriminator.state_dict(), os.path.join(args.outputs_dir, f\"final_d.pth\"))\n    torch.save(generator.state_dict(), os.path.join(args.outputs_dir, f\"final_g.pth\"))\n        \n    return g_losses, d_losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"class ARGS():\n    outputs_dir = 'output'\n    latent_space = 100\n    batch_size = 64\n    epochs = 50\n    b1 = 0.5\n    b2 = 0.9\n    lr = 0.002\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    outputs_dir = os.path.join(outputs_dir,  \"GAN\")\n    if not os.path.exists(outputs_dir):\n        os.makedirs(outputs_dir)\n    \n    seed = 42\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \n    # for the current configuration, so as to optimize the operation efficiency.\n    cudnn.benchmark = True\n    # Ensure that every time the same input returns the same result.\n    cudnn.deterministic = True\n    \nargs = ARGS()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"def main(args):\n    loader = load_dataset(args.batch_size)\n    print('Loaded dataset successfully')\n    generator, discriminator = build_model(args)\n    print('Built model successfully')\n    loss_fn = loss_function(args)\n    print('Define loss function succesfully')\n    optimizer_g, optimizer_d = optimizer(generator, discriminator,args)\n    print('Define all optimization function successfully')\n    \n    # training\n    g_losses, d_losses, img_list = trainer(loader, generator, discriminator, optimizer_g, optimizer_d, loss_fn, args)\n    \n    plt.figure(figsize=(10,5))\n    plt.title(\"Generator and Discriminator Loss During Training\")\n    plt.plot(g_losses,label=\"G\")\n    plt.plot(d_losses,label=\"D\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main(args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}