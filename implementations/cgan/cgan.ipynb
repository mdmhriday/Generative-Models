{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\nimport PIL.Image as Image\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import Tensor\n\nimport torchvision\nfrom torchvision import datasets, transforms\nimport torchvision.utils as vutils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Architecture","metadata":{}},{"cell_type":"code","source":"# Class : Generator\nclass Generator(nn.Module):\n    def __init__(self,\n                 latent_space : int = 100,\n                 image_size : int = 28,\n                 channels : int = 1,\n                 num_classes : int = 10,\n                 ) -> None:\n        super(Generator, self).__init__()\n        self.image_size = image_size\n        self.channels = channels\n        self.num_classes = num_classes\n        \n        self.label_embedding = nn.Embedding(num_classes, num_classes)\n        \n        def block(in_planes, out_planes, normalize = True):\n            layers = [nn.Linear(in_planes, out_planes)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_planes))\n            layers.append(nn.LeakyReLU(0.2, inplace = True))\n            \n            return layers\n        \n        self.blocks = nn.Sequential(\n            *block(latent_space + num_classes, 128, normalize = False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, channels * image_size * image_size),\n            nn.Tanh()\n        )\n        \n        \n    def forward(self, x : Tensor,\n                labels : list = None,\n               ) -> Tensor:\n        \n        x = torch.cat([x, self.label_embedding(labels)], dim = 1)\n        out = self.blocks(x)\n        out = out.view(out.size(0), *(1, 28, 28))\n        return out\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class : Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self,\n                 image_size : int = 28,\n                 channels : int = 1,\n                 num_classes : int = 10,\n                ) -> None:\n        super(Discriminator, self).__init__()\n        \n        self.label_embedding = nn.Embedding(num_classes, num_classes)\n        \n        self.block = nn.Sequential(\n            nn.Linear(channels * image_size * image_size + num_classes, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Linear(256, 128),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x : Tensor,\n                labels : list = None,\n               ) -> Tensor:\n        \n        x = torch.flatten(x, 1)\n        out = torch.cat([x, self.label_embedding(labels)], dim = 1)\n        out = self.block(out)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def build_model(args):\n    generator = Generator().to(args.device)\n    discriminator = Discriminator().to(args.device)\n    \n    return generator, discriminator\n    \n    \ndef load_dataset(args):\n    \"\"\"\n    Make a directory : ./data/mnist\n    Download dataset into the directory and //\n    use dataloader to load the dataset with //\n    given batch size\n    ---------------\n    return dataloader\n    \"\"\"\n    os.makedirs(\"./data/mnist\", exist_ok = True)\n    dataloader = DataLoader(\n        datasets.MNIST(\"./data/mnist\",\n                       train = True,\n                       transform = transforms.Compose([\n                           transforms.Resize(28),\n                           transforms.ToTensor(),\n                           transforms.Normalize([0.5,],[0.5,]),\n                       ]),\n                      download = True,),\\\n        batch_size = args.batch_size,\n        shuffle = True,\n    )\n    \n    return dataloader\n        \ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        if m.bias.data is not None:\n            m.bias.data.fill_(0.01)\n    elif isinstance(m, nn.BatchNorm1d):\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        if m.bias.data is not None:\n            m.bias.data.fill_(0.01)\n\ndef loss_function(args):\n    \n    loss = nn.BCELoss().to(args.device)\n    return loss\n\ndef optimizer(generator, discriminator, args):\n    \n    optimizer_g = optim.Adam(generator.parameters(),\n                             lr = args.lr, betas = (args.b1, args.b2))\n    optimizer_d = optim.Adam(discriminator.parameters(),\n                             lr = args.lr, betas = (args.b1, args.b2))\n    \n    return optimizer_g, optimizer_d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"def trainer(loader,\n            generator,\n            discriminator,\n            optimizer_g,\n            optimizer_d, \n            loss_fn,\n            args):\n    \n    ######################################################################\n    #       Training Conditional Generative Adversarial Networks\n    ######################################################################\n    \n    # List to plot losses\n    g_losses = []\n    d_losses = []\n    \n    start = datetime.now()\n    \n    # Set model in training mode\n    generator.train()\n    discriminator.train()\n    \n    for epoch in range(1, args.epochs):\n        for index, (image, label) in enumerate(loader):\n            image = image.view(-1, 784)\n            b_size = image.shape[0]\n            \n            image = image.to(args.device)\n            label = label.to(args.device)\n            \n            # create ground truth : set real sample level to 1 and fake sample level to 0\n            real_sample = torch.full((b_size, 1), 1, dtype= image.dtype, device = args.device)\n            fake_sample = torch.full((b_size, 1), 0, dtype= image.dtype, device = args.device)\n            \n            # noise sample and label for generator input\n            noise = torch.randn([b_size, 100], device = args.device)\n            gen_label = torch.randint(1, 10, (b_size,), device = args.device)\n            ######################################################################\n            # Train Discriminator max E(x) log(D(x)) + E(z) [log(1 - D(z))]\n            ######################################################################\n            \n            # Initialize discriminator model with zero gradient\n            discriminator.zero_grad()\n    \n            # Calculate loss of discriminator on real data\n            output = discriminator(image, label)\n            loss_real_d = loss_fn(output, real_sample)\n            # Calculate gradiant of discriminator in backward pass\n            loss_real_d.backward()\n            d_x = output.mean().item()\n            \n            # Generate fake images and fake label\n            fake = generator(noise, gen_label)\n            \n            # Calculate loss of discriminator on fake data\n            fake_output = discriminator(fake.detach(), gen_label)\n            loss_fake_d = loss_fn(fake_output, fake_sample)\n            # Calculate gradient of discriminator in backward pass\n            loss_fake_d.backward()\n            d_g_z1 = loss_fake_d.mean().item()\n            # Calculate loss of discriminator on both real and fake data\n            loss_d = loss_real_d + loss_fake_d / 2\n            # Update dicriminator weights\n            optimizer_d.step()\n            \n            ######################################################################\n            # Train Generator min E(z) [log(1 - D(z))]\n            ######################################################################\n            \n            # Initialize generator model with zero gradient\n            generator.zero_grad()\n            \n            # Calculate loss of generator based on discriminator output\n            fake_output = discriminator(fake, gen_label)\n            loss_g = loss_fn(fake_output, real_sample)\n            # Calculate gradient of generator in backward pass\n            loss_g.backward()\n            d_g_z2 = output.mean().item() \n            # Update weights of generator model\n            optimizer_g.step()\n            # save images to see training stability\n            if index == 0:\n                vutils.save_image(vutils.make_grid(fake.detach().cpu().view(-1, *(1, 28, 28)),\n                                                   normalize = True),\n                                  os.path.join(args.outputs_dir, f\"fake_image_{epoch}.jpg\"))\n                vutils.save_image(vutils.make_grid(image.detach().cpu().view(-1, *(1, 28, 28)),\n                                                   normalize = True),\n                                  os.path.join(args.outputs_dir, f\"real_image_{epoch}.jpg\"))\n            \n            if index % 10 == 0 or index == len(loader):\n                print(f\"Train stage: adversarial \"\n                      f\"Epoch[{epoch:04d}/{args.epochs:04d}]({index:05d}/{len(loader):05d})\"\n                      f\"D Loss: {loss_d.item():.6f} G Loss: {loss_g.item():.6f}\"\n                      f\"D_X: {d_x:.6f} D_G_Z1/ D_G_Z2: {d_g_z1:.6f}/{d_g_z2:.6f}.\")\n            \n            # save losses for plotting later\n            g_losses.append(loss_g.item())\n            d_losses.append(loss_d.item())\n        \n        print(\"Training complete in: \" + str(datetime.now() - start))\n    return g_losses, d_losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ARGS():\n    outputs_dir = 'output'\n    latent_space = 100\n    batch_size = 64\n    epochs = 50\n    b1 = 0.5\n    b2 = 0.9\n    lr = 0.002\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    outputs_dir = os.path.join(outputs_dir,  \"cgan\")\n    if not os.path.exists(outputs_dir):\n        os.makedirs(outputs_dir)\n    \n    seed = 42\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    \nargs = ARGS()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"def main(args):\n    loader = load_dataset(args)\n    print('Loaded dataset successfully')\n    generator, discriminator = build_model(args)\n    print('Built model successfully')\n    generator.apply(init_weights)\n    discriminator.apply(init_weights)\n    print('Weights initialize successfully')\n    loss_fn = loss_function(args)\n    print('Define loss function succesfully')\n    optimizer_g, optimizer_d = optimizer(generator, discriminator,args)\n    print('Define all optimization function successfully')\n    \n    # training\n    g_losses, d_losses = trainer(loader, generator, discriminator, optimizer_g, optimizer_d, loss_fn, args)\n    \n    plt.figure(figsize=(10,5))\n    plt.title(\"Generator and Discriminator Loss During Training\")\n    plt.plot(g_losses,label=\"G\")\n    plt.plot(d_losses,label=\"D\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main(args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}